---
title: "Practical Bash"
author: "Prof. Harbert"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  html_document:
    theme: united
    highlight: tango
    df_print: paged
    toc: true
    toc_float: true
    toc_depth: 4
  
---


```{r setup, include=FALSE}
library(reticulate)
use_python('/path/to/python') #EDIT
knitr::opts_chunk$set(echo = TRUE)
```

[home](https://rsh249.github.io/applied_bioinformatics)

# Developing your project workflow

Today we will look at how we run programs on the Unix commandline. As most of the bioinformatic software you will be using for your Projects is run this way, it is important to be comfortable working this way. It is also a good time to talk about managing your projects for documentation and reproducibility.

# Bash

The scripts you will be writing today are in a common format that ties together many bioinformatics workflows. This is also a common format for submitting computational jobs in High Performance Computing (HPC) environments that enforce queueing software.

"Bash" scripting is a common name for any Unix shell script. Some people will use it specifically to refer to the Bourne Again SHell, but you may encounter other variants of the Unix shell. We will use Bash but most of what we use will be fully transferable if you ever encounter another shell.

Unix commands that we have seen so far run programs ('R', 'top') interact with the file system ('cd', 'ls', 'cp') and read files ('grep', 'wc', 'cat'). 

We will use bash scripts to explore the data for your projects today and ultimately you will build an entire analysis pipeline to do your project. We will try to keep together but some of your data may be incompatible with some of this exploration. Also, if your files are particularly large it may take more time than expected.

## Exploring Sequence Data

Things I want to know about your data files:

+ How many files are there?
+ What size is each file? What's the total amount of storage used?
+ How many sequence reads are there in each? ('wc' or 'grep')
+ Peek at quality scores (try fastqc)
+ What is the read-length distribution? ('awk')

Your project data are on the server at:

```{bash, eval=F}
cd /usr/share/data/proj_data
ls #Look for your project codename and 'cd' into that directory.

#in your directory try:
ls -lh

```



# Scripting

Anything that you can run by typing at the Unix command prompt (i.e., *most* bioinformatics software) can, and should, be written in a Bash script. This way you will be able to manage complex commands and workflows without worrying about mis-typing a command or getting things out of order. Also, you can automate your analysis; as soon as one part is done the next will start to run.

## Setting up your script

Go back to your user directory:

```{bash, eval=F}
cd ~
mkdir project_codename
cd project_codename
```

Create a new file using 'nano'

```{bash, eval=F}
nano bash_proj

```

In the nano prompt type:

```{bash, eval=F}
#!/bin/bash

#This is a comment

echo "This is a test"

```

And then type "Ctrl+o" to save changes and "Ctrl+x" to exit. (You can type just "Ctrl+x" and answere "Y" when asked to save)

Then to RUN the script:

```{bash, eval=F}
bash bash_proj

```

You should see output that says "This is a test"

We can now add any Unix command to the bash script and they will all be run in sequence when we run the script.

For example open the script and modify to be:

```{bash, eval=T}
#!/bin/bash

#This is a comment

echo "This is a test"

ls -lh


```

## Script Organization

A good script is designed to be re-used. I don't care about how efficient your code is or if it is formatted cleanly. What makes a document like this useful to yourself and others is whether it can be understood. This means that you need to clearly provide the details of how to use the script and what it is doing along the way.

Open your script again and add comment lines (Start each line with '#') to the top with the following information about your project:

+ Your name
+ Current Term/Year
+ Citation of the paper you are emulating 
+ Source of data (SRA or ENA id preferred but URL good too)
+ Pre-requisite programs list

## Setting Variables

A good starting point of any script is to set up bash variables with the path to your data (and eventually any reference data).

You set a bash variable like this: (try it on the command line first)

```{bash, eval=F}
var='value'
```

And call as:

```{bash, eval=F}
echo $var
```

Add the path to your data (e.g., '/usr/share/.....') to the top of the script

```{bash, eval=F}
datapath='/usr/share/data/proj_data/thale_cress'

#then check it with:
echo $datapath

#and try and use it in a command:
ls -lh $datapath

```

## Testing

As you work on your script and add commands it is usually a good idea to test frequently.

Recall that you run a script as:

```{bash, eval=F}
bash bash_proj
```

## Logging

It can be useful to create a log file and output important information or milestones to the log.

In your script try:

```{bash, eval=F}
logit='log.txt'
echo "Logging to $logit"

touch $logit #Not strictly necessary, but can be useful to create an empty file early on.

echo "Output test" > $logit #the > symbol redirects the echo output to the file indicated

```

Then look at the "log.txt" file.

```{bash, eval=F}
cat 'log.txt'
```

Keep this in mind as you work on your pipeline. Logging major landmarks after each program can give vital information about your analysis WHEN it fails when you're not watching.

# Exploring your data 

From here out, keep notes about what each command does. Your blog post this week will involve adding this to your bash script and writing up how and why it works.

## File accounting

How many files do you have in your directory? (We will worry about whether we have everything there later)


```{bash, eval=F}
datapath='/usr/share/data/proj_data/thale_cress/'
ls -l $datapath | wc -l
```

We want to save the number of files as a bash variable:

```{bash, eval=F}
datapath='/usr/share/data/proj_data/thale_cress/'
numfiles=$(ls -l $datapath | wc -l)
echo $numfiles
```


## File size

We can get the file size of each file with:

```{bash, eval=F}
du -h $datapath/*.fastq.gz
```

And can parse the file sizes using 'awk' (more on this later):
Print the file name first, then the file size. Or any other combination of info you want here.

```{bash, eval=F}
du -h $datapath/*.fastq.gz | awk '{print $2 $1}'

```






## Number of records

## Quality scores

## Read Length Distribution




[home](https://rsh249.github.io/applied_bioinformatics)
